# ДЗ: Отказоустойчивое хранилище на Ceph

## Цель проекта

Развернуть отказоустойчивый кластер распределенного хранилища данных на базе Ceph с поддержкой блочного (RBD), файлового (CephFS) и объектного доступа.

## Архитектура кластера

### Компоненты Ceph

**Monitors (MON)** - 3 узла
- Управляют состоянием кластера
- Хранят карту кластера (cluster map)
- Обеспечивают кворум для принятия решений

**OSD (Object Storage Daemon)** - минимум 3 узла
- Хранят данные объектов
- Реплицируют данные между OSD
- Обеспечивают отказоустойчивость

**MDS (Metadata Server)** - 1-2 узла (для CephFS)
- Управляют метаданными файловой системы
- Обеспечивают доступ к CephFS

**Client Nodes** - 2-3 узла
- Подключаются к кластеру как клиенты
- Используют RBD тома и монтируют CephFS

### Топология

```
┌─────────────────────────────────────────────────────────┐
│                    Yandex Cloud                          │
│                                                           │
│  ┌────────────────────────────────────────────────────┐  │
│  │         VPC Network: otus-network                 │  │
│  │         Subnet: otus-subnet                       │  │
│  │                                                     │  │
│  │  ┌──────────────┐  ┌──────────────┐              │  │
│  │  │ Monitor-1    │  │ Monitor-2    │              │  │
│  │  │ (MON)        │  │ (MON)        │              │  │
│  │  └──────────────┘  └──────────────┘              │  │
│  │                                                     │  │
│  │  ┌──────────────┐  ┌──────────────┐              │  │
│  │  │ Monitor-3    │  │ OSD-1         │              │  │
│  │  │ (MON)        │  │ (OSD + Disk)  │              │  │
│  │  └──────────────┘  └──────────────┘              │  │
│  │                                                     │  │
│  │  ┌──────────────┐  ┌──────────────┐              │  │
│  │  │ OSD-2        │  │ OSD-3         │              │  │
│  │  │ (OSD + Disk) │  │ (OSD + Disk)  │              │  │
│  │  └──────────────┘  └──────────────┘              │  │
│  │                                                     │  │
│  │  ┌──────────────┐  ┌──────────────┐              │  │
│  │  │ MDS-1        │  │ Client-1     │              │  │
│  │  │ (Metadata)   │  │ (RBD+CephFS) │              │  │
│  │  └──────────────┘  └──────────────┘              │  │
│  │                                                     │  │
│  │  ┌──────────────┐                                 │  │
│  │  │ Client-2     │                                 │  │
│  │  │ (RBD+CephFS) │                                 │  │
│  │  └──────────────┘                                 │  │
│  └─────────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────────┘
```

## Расчет хранилища и Placement Groups (PG)

### Исходные данные

**Общий объем хранилища (учебный стенд):**
- 3 OSD узла
- По 1 диску 10 GB на каждом OSD
- **Общий объем: 30 GB (сырые данные)**

**Фактор репликации:** 3 (для демонстрации отказоустойчивости)

**Доступный объем после репликации:**
- 30 GB / 3 = **10 GB** доступного пространства (для наших тестовых данных ≤ 500 MB этого более чем достаточно)

### Распределение по пулам

**RBD (RADOS Block Device):**
- 50-100% от доступного объема
- Выбираем: **~70% ≈ 7 GB**
- Использование: блочные устройства для тестовых сервисов/VM

**CephFS (Ceph File System):**
- 30% от доступного объема
- Выбираем: **~30% ≈ 3 GB**
- Использование: общее файловое хранилище для тестовых данных (≤ 500 MB)

### Расчет Placement Groups (PG)

**Формула расчета PG:**
```
Total PG = (Total OSDs × 100) / Replication Factor
```

**Для нашего кластера:**
- Total OSDs = 3
- Replication Factor = 3
- Total PG = (3 × 100) / 3 = **100 PG**

**Распределение PG по пулам:**

1. **RBD pool:**
   - 70% от общего объема → ~70 PG
   - Округляем до ближайшей степени 2: **64 PG** (2^6)

2. **CephFS pool:**
   - 30% от общего объема → ~30 PG
   - Округляем до ближайшей степени 2: **32 PG** (2^5)

3. **Metadata pool для CephFS:**
   - Обычно 1/10 от data pool
   - 32 / 10 ≈ **4 PG** (округляем до 4)

**Итого:**
- RBD: 64 PG
- CephFS data: 32 PG
- CephFS metadata: 4 PG
- **Всего: 100 PG**

### Логика расчета PG

**Почему степени 2?**
- Ceph использует алгоритм CRUSH (Controlled Replication Under Scalable Hashing)
- Для эффективного распределения данных PG должны быть кратны степени 2
- Это обеспечивает равномерное распределение данных по OSD

**Почему 100 PG на 3 OSD?**
- Рекомендация: 50-100 PG на OSD для небольших кластеров
- Для 3 OSD: 3 × 50 = 150 PG (максимум)
- Мы используем 100 PG, что находится в оптимальном диапазоне

**Преимущества правильного количества PG:**
- Равномерное распределение данных
- Быстрое восстановление после сбоев
- Эффективное использование ресурсов

## Технологический стек

| Компонент | Технология | Версия/Назначение |
|-----------|-----------|-------------------|
| **Инфраструктура** | Terraform | IaC для Yandex Cloud |
| **Автоматизация** | Ansible | Configuration Management |
| **Ceph** | Ceph | Pacific/Quincy (LTS) |
| **Ceph Ansible** | ceph-ansible | Автоматизация развертывания |
| **OS** | Ubuntu | 20.04 LTS |
| **Cloud Provider** | Yandex Cloud | Compute Instances |

## Структура проекта

```
lesson_36/
├── terraform/                    # Инфраструктура Yandex Cloud
│   ├── main.tf                   # Основная конфигурация (VM, SG)
│   ├── variables.tf              # Переменные Terraform
│   ├── outputs.tf                 # Выводы (IP адреса)
│   └── templates/
│       └── inventory.tpl          # Шаблон Ansible inventory
│
├── ansible/                       # Автоматизация через Ansible
│   ├── ansible.cfg                # Конфигурация Ansible
│   ├── inventory                  # Инвентарь хостов (генерируется)
│   ├── group_vars/
│   │   └── all/
│   │       └── main.yml          # Глобальные переменные
│   ├── playbooks/
│   │   ├── deploy-ceph.yml       # Развертывание Ceph кластера
│   │   ├── configure-pools.yml   # Настройка пулов
│   │   └── setup-clients.yml     # Настройка клиентских машин
│   └── roles/
│       ├── ceph-mon/              # Роль для мониторов
│       ├── ceph-osd/              # Роль для OSD
│       ├── ceph-mds/              # Роль для MDS
│       └── ceph-client/           # Роль для клиентов
│
├── scripts/
│   └── check-ceph-status.sh      # Скрипт проверки состояния кластера
├── scripts/
│   └── check-ceph-status.sh      # Скрипт проверки состояния кластера
├── README.md                      # Этот файл (документация)
├── USAGE.md                       # Руководство по использованию Ceph
├── APPLY.md                       # Инструкции по развертыванию
├── QUICKSTART.md                  # Быстрый старт
└── ДЗ.md                          # Описание домашнего задания
```

## Порты Ceph

| Служба | Порт | Протокол | Назначение |
|--------|------|----------|------------|
| **MON** | 6789 | TCP | Monitor daemon |
| **OSD** | 6800-7300 | TCP | OSD daemon (множество портов) |
| **MDS** | 6800 | TCP | Metadata Server |
| **RGW** | 7480 | HTTP | RADOS Gateway (опционально) |

## Развертывание

### Предпосылки

- Yandex Cloud CLI настроен (переменные окружения: `YC_TOKEN`, `YC_CLOUD_ID`, `YC_FOLDER_ID`)
- Terraform установлен (версия >= 1.6.0)
- Ansible установлен (версия >= 2.9)
- SSH ключ доступен (`~/.ssh/id_ed25519.pub`)

### Шаги развертывания

1. **Создание инфраструктуры (Terraform)**
   ```bash
   cd terraform
   terraform init
   terraform plan
   terraform apply
   ```

2. **Развертывание Ceph кластера (Ansible)**
   ```bash
   cd ..
   ansible-playbook -i terraform/inventory ansible/playbooks/deploy-ceph.yml
   ```

3. **Настройка пулов**
   ```bash
   ansible-playbook -i terraform/inventory ansible/playbooks/configure-pools.yml
   ```

4. **Настройка клиентских машин**
   ```bash
   ansible-playbook -i terraform/inventory ansible/playbooks/setup-clients.yml
   ```

## Аварийные сценарии

### 3.1. Split-brain

**Описание:** Ситуация, когда два OSD имеют разные версии одного и того же объекта.

**Отработка:**
- Генерация split-brain через принудительное отключение сети
- Проверка состояния кластера
- Разрешение конфликта через `ceph pg repair`

### 3.2. Сбой OSD узла

**Описание:** Отказ одного из OSD узлов и его замена.

**Отработка:**
- Остановка OSD daemon
- Вывод узла из кластера
- Добавление нового OSD узла
- Проверка восстановления данных

### 3.3. Сбой серверной/дата-центра

**Описание:** Симуляция отказа целой зоны доступности.

**Отработка:**
- Остановка всех узлов в одной зоне
- Проверка работоспособности сервисов
- Восстановление после возврата узлов

### 3.4. Расширение кластера

**Описание:** Добавление 2+ OSD узлов и перерасчет PG.

**Отработка:**
- Добавление новых OSD узлов
- Перерасчет PG для оптимального распределения
- Объяснение логики перерасчета

### 3.5. Уменьшение кластера

**Описание:** Удаление 1 OSD узла и перерасчет PG.

**Отработка:**
- Вывод OSD из кластера
- Перерасчет PG
- Объяснение логики перерасчета

## Проверка работоспособности

### Использование скрипта проверки

```bash
# Запуск скрипта проверки состояния кластера
./scripts/check-ceph-status.sh

# Или с полным путем
bash lesson_36/scripts/check-ceph-status.sh
```

Скрипт проверяет:
- Подключение к кластеру
- Общий статус кластера
- Статус мониторов (MON)
- Статус OSD узлов
- Статус пулов (Pools)
- Статус Placement Groups (PG)
- Статус CephFS (если настроен)
- Здоровье кластера

### Ручная проверка состояния кластера

```bash
# Статус кластера
ceph -s

# Статус OSD
ceph osd tree

# Статус пулов
ceph df
```

### Проверка RBD томов

```bash
# Список томов
rbd list

# Информация о томе
rbd info <pool>/<image>
```

### Проверка CephFS

```bash
# Статус CephFS
ceph fs status

# Монтирование
mount -t ceph <mon-ip>:6789:/ /mnt/cephfs -o name=admin,secret=<key>
```

## Использование Ceph

Подробное руководство по использованию Ceph доступно в файле **[USAGE.md](USAGE.md)**.

Основные разделы:
- Работа с RBD (RADOS Block Device)
- Работа с CephFS
- Управление пулами
- Мониторинг и диагностика
- Типичные операции и примеры

## Безопасность

- Security Groups ограничивают доступ к портам Ceph
- Использование ключей аутентификации для клиентов
- Рекомендация: настроить TLS для production окружений

## Производительность

- Фактор репликации 3 обеспечивает высокую надежность
- Распределение данных по 3 OSD узлам
- Оптимальное количество PG для равномерного распределения



